{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIa9S21P9DhrZTltKEm35t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LorenzoCorbinelli/MLSA-project/blob/main/Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment setup"
      ],
      "metadata": {
        "id": "8xzoEt7XJKxF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQk-cRIGI7FZ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import RobertaTokenizer, RobertaConfig, AutoModel, DataCollatorForLanguageModeling, RobertaForMaskedLM\n",
        "from datasets import Dataset as ds\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "from torch.optim import Adam"
      ],
      "metadata": {
        "id": "tODInvViJPpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the dataset"
      ],
      "metadata": {
        "id": "yCZlpt-pJV_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Download the dataset\n",
        "!wget http://files.srl.inf.ethz.ch/data/py150_files.tar.gz"
      ],
      "metadata": {
        "id": "pXfouBatJS3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvzf py150_files.tar.gz # unzip the folder containing the dataset"
      ],
      "metadata": {
        "id": "S_W4wutyJZVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvzf data.tar.gz # unzip the dataset"
      ],
      "metadata": {
        "id": "4B8HAofAJa_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After unzipping the data.tar.gz folder, we retrieve the python files, removing all the comments because they are not usefull for our purposes"
      ],
      "metadata": {
        "id": "EWrTCbKFJfOq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract all the source code of the python files into a dataframe. One file is reported into a single line, including the reference of the file itself"
      ],
      "metadata": {
        "id": "4reI8vKJJgH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_python_files(root_dir):\n",
        "    \"\"\"Loads all Python files in a directory recursively into a DataFrame.\"\"\"\n",
        "    all_files = []\n",
        "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
        "        for filename in filenames:\n",
        "            if filename.endswith(\".py\"):  # take only python files\n",
        "                filepath = os.path.join(dirpath, filename)\n",
        "                try:\n",
        "                    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                        content = f.readlines() # read the file line by line\n",
        "                        filtered_content = []\n",
        "                        inside_triple_quotes = False\n",
        "                        for line in content:\n",
        "                            stripped_line = line.strip()\n",
        "                            if '\"\"\"' in stripped_line:\n",
        "                                if stripped_line.count('\"\"\"') == 2:\n",
        "                                    continue  # Ignore lines with both opening and closing triple quotes\n",
        "                                inside_triple_quotes = not inside_triple_quotes\n",
        "                                continue\n",
        "                            if inside_triple_quotes or stripped_line.startswith('#'): # ignore line that starts with # or check if I am inside a multiline comment\n",
        "                                continue\n",
        "                            filtered_content.append(line)\n",
        "                        all_files.append({'filepath': filepath, 'snippet_of_code': ''.join(filtered_content)})\n",
        "                except Exception as e:\n",
        "                    print(f\"Error reading file {filepath}: {e}\")\n",
        "    return pd.DataFrame(all_files)\n"
      ],
      "metadata": {
        "id": "H8ZN_84cJcsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/data\"\n",
        "df_python_files = load_python_files(data_dir)"
      ],
      "metadata": {
        "id": "Fpk-_U2DJjWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_python_files[\"snippet_of_code\"].iloc[0:2000] # take only the first 2000 snippets for training\n",
        "df_eval = df_python_files[\"snippet_of_code\"].iloc[2000:3000] # take 1000 snippets for evaluation"
      ],
      "metadata": {
        "id": "LszRrFRUJmT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "rq9SC67XJm2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base-mlm')\n",
        "# tokenizer arguments to properly handle the tokenization of the snippets\n",
        "tokenizer_kwargs = dict(truncation=True, padding=True, max_length=500, add_special_tokens=True)"
      ],
      "metadata": {
        "id": "mmYX8ocuJpVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "We have used DataCollatorForLanguageModeling in order to tokenize the dataset and mask some tokens\n",
        "It will automatically generate the labels for the masked tokens.\n",
        "For the tokens not masked the label will be -100\n",
        "'''\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
      ],
      "metadata": {
        "id": "GdLwmd7AJq84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_dataset(dataset, tokenizer, **kwargs):\n",
        "    token_ids = tokenizer(dataset, return_tensors='pt', **kwargs)\n",
        "    return token_ids"
      ],
      "metadata": {
        "id": "BPeoTvP7Jsw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tensor_dataset = tokenize_dataset(df_train.to_list(), tokenizer, **tokenizer_kwargs)\n",
        "eval_tensor_dataset = tokenize_dataset(df_eval.to_list(), tokenizer, **tokenizer_kwargs)\n",
        "\n",
        "datasetTrain = ds.from_dict(train_tensor_dataset)\n",
        "datasetEval = ds.from_dict(eval_tensor_dataset)\n",
        "\n",
        "datasetTrain.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "datasetEval.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "\n",
        "generator = torch.Generator()\n",
        "train_loader = DataLoader(datasetTrain, batch_size=4, shuffle=True, generator=generator, collate_fn=data_collator)\n",
        "eval_loader = DataLoader(datasetEval, batch_size=4, collate_fn=data_collator)"
      ],
      "metadata": {
        "id": "CHuL6bi4JvYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RobertaForMaskedLM.from_pretrained('microsoft/codebert-base-mlm')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "optimizer = Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(2):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        inputs = {key: val.to(device) for key, val in batch.items() if key in tokenizer.model_input_names}\n",
        "        inputs['labels'] = batch['labels'].to(device)\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs.loss\n",
        "        print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch + 1} completed\")\n"
      ],
      "metadata": {
        "id": "bpFMKzxDJxCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation loop\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in eval_loader:\n",
        "        input = {key: val.to(device) for key, val in batch.items()}\n",
        "        outputs = model(**input)\n",
        "        print(outputs.loss)"
      ],
      "metadata": {
        "id": "4j38LDW2JzSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model\n",
        "directory = 'path_to_the_model_directory'\n",
        "\n",
        "model.save_pretrained(directory)\n",
        "tokenizer.save_pretrained(directory)"
      ],
      "metadata": {
        "id": "5_7ZFXpeKScr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}