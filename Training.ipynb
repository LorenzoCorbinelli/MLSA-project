{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LorenzoCorbinelli/MLSA-project/blob/chunking/Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment setup"
      ],
      "metadata": {
        "id": "8xzoEt7XJKxF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "yQk-cRIGI7FZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b312146d-2da3-45ad-c3b0-70428e583929"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import RobertaTokenizer, RobertaConfig, AutoModel, DataCollatorForLanguageModeling, RobertaForMaskedLM\n",
        "from datasets import Dataset as ds\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "from torch.optim import Adam"
      ],
      "metadata": {
        "id": "tODInvViJPpb"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the dataset"
      ],
      "metadata": {
        "id": "yCZlpt-pJV_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Download the dataset\n",
        "!wget http://files.srl.inf.ethz.ch/data/py150_files.tar.gz"
      ],
      "metadata": {
        "id": "pXfouBatJS3-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a9f8187-be55-4a19-ab37-7092be34092a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "--2025-02-08 17:38:49--  http://files.srl.inf.ethz.ch/data/py150_files.tar.gz\n",
            "Resolving files.srl.inf.ethz.ch (files.srl.inf.ethz.ch)... 129.132.114.90\n",
            "Connecting to files.srl.inf.ethz.ch (files.srl.inf.ethz.ch)|129.132.114.90|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://files.sri.inf.ethz.ch/data/py150_files.tar.gz [following]\n",
            "--2025-02-08 17:38:49--  https://files.sri.inf.ethz.ch/data/py150_files.tar.gz\n",
            "Resolving files.sri.inf.ethz.ch (files.sri.inf.ethz.ch)... 129.132.114.90\n",
            "Connecting to files.sri.inf.ethz.ch (files.sri.inf.ethz.ch)|129.132.114.90|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 199067128 (190M) [application/gzip]\n",
            "Saving to: ‘py150_files.tar.gz.1’\n",
            "\n",
            "py150_files.tar.gz.  41%[=======>            ]  78.67M  18.5MB/s    eta 9s     "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvzf py150_files.tar.gz # unzip the folder containing the dataset"
      ],
      "metadata": {
        "id": "S_W4wutyJZVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvzf data.tar.gz # unzip the dataset"
      ],
      "metadata": {
        "id": "4B8HAofAJa_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After unzipping the data.tar.gz folder, we retrieve the python files, removing all the comments because they are not usefull for our purposes"
      ],
      "metadata": {
        "id": "EWrTCbKFJfOq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract all the source code of the python files into a dataframe. One file is reported into a single line, including the reference of the file itself"
      ],
      "metadata": {
        "id": "4reI8vKJJgH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_python_files(root_dir):\n",
        "    \"\"\"Loads all Python files in a directory recursively into a DataFrame.\"\"\"\n",
        "    all_files = []\n",
        "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
        "        for filename in filenames:\n",
        "            if filename.endswith(\".py\"):  # take only python files\n",
        "                filepath = os.path.join(dirpath, filename)\n",
        "                try:\n",
        "                    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                        content = f.readlines() # read the file line by line\n",
        "                        filtered_content = []\n",
        "                        inside_triple_quotes = False\n",
        "                        for line in content:\n",
        "                            stripped_line = line.strip()\n",
        "                            if '\"\"\"' in stripped_line:\n",
        "                                if stripped_line.count('\"\"\"') == 2:\n",
        "                                    continue  # Ignore lines with both opening and closing triple quotes\n",
        "                                inside_triple_quotes = not inside_triple_quotes\n",
        "                                continue\n",
        "                            if inside_triple_quotes or stripped_line.startswith('#'): # ignore line that starts with # or check if I am inside a multiline comment\n",
        "                                continue\n",
        "                            filtered_content.append(line)\n",
        "                        all_files.append({'filepath': filepath, 'snippet_of_code': ''.join(filtered_content)})\n",
        "                except Exception as e:\n",
        "                    print(f\"Error reading file {filepath}: {e}\")\n",
        "    return pd.DataFrame(all_files)\n"
      ],
      "metadata": {
        "id": "H8ZN_84cJcsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/data\"\n",
        "df_python_files = load_python_files(data_dir)"
      ],
      "metadata": {
        "id": "Fpk-_U2DJjWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_python_files[\"snippet_of_code\"].iloc[0:2000] # take only the first 2000 snippets for training\n",
        "df_eval = df_python_files[\"snippet_of_code\"].iloc[2000:3000] # take 1000 snippets for evaluation"
      ],
      "metadata": {
        "id": "LszRrFRUJmT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "rq9SC67XJm2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base-mlm')\n",
        "# tokenizer arguments to properly handle the tokenization of the snippets\n",
        "tokenizer_kwargs = dict(truncation=True, padding=True, max_length=500, add_special_tokens=True)"
      ],
      "metadata": {
        "id": "mmYX8ocuJpVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "We have used DataCollatorForLanguageModeling in order to tokenize the dataset and mask some tokens\n",
        "It will automatically generate the labels for the masked tokens.\n",
        "For the tokens not masked the label will be -100\n",
        "'''\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
      ],
      "metadata": {
        "id": "GdLwmd7AJq84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_dataset(dataset, tokenizer, **kwargs):\n",
        "    token_ids = tokenizer(dataset, return_tensors='pt', **kwargs)\n",
        "    return token_ids"
      ],
      "metadata": {
        "id": "BPeoTvP7Jsw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tensor_dataset = tokenize_dataset(df_train.to_list(), tokenizer, **tokenizer_kwargs)\n",
        "eval_tensor_dataset = tokenize_dataset(df_eval.to_list(), tokenizer, **tokenizer_kwargs)\n",
        "\n",
        "datasetTrain = ds.from_dict(train_tensor_dataset)\n",
        "datasetEval = ds.from_dict(eval_tensor_dataset)\n",
        "\n",
        "datasetTrain.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "datasetEval.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "\n",
        "generator = torch.Generator()\n",
        "train_loader = DataLoader(datasetTrain, batch_size=4, shuffle=True, generator=generator, collate_fn=data_collator)\n",
        "eval_loader = DataLoader(datasetEval, batch_size=4, collate_fn=data_collator)"
      ],
      "metadata": {
        "id": "CHuL6bi4JvYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RobertaForMaskedLM.from_pretrained('microsoft/codebert-base-mlm')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "optimizer = Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(2):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        inputs = {key: val.to(device) for key, val in batch.items() if key in tokenizer.model_input_names}\n",
        "        inputs['labels'] = batch['labels'].to(device)\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs.loss\n",
        "        print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch + 1} completed\")\n"
      ],
      "metadata": {
        "id": "bpFMKzxDJxCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation loop\n",
        "'''\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in eval_loader:\n",
        "        input = {key: val.to(device) for key, val in batch.items()}\n",
        "        outputs = model(**input)\n",
        "        print(outputs.loss)'''"
      ],
      "metadata": {
        "id": "4j38LDW2JzSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model\n",
        "'''\n",
        "directory = 'path_to_the_model_directory'\n",
        "\n",
        "model.save_pretrained(directory)\n",
        "tokenizer.save_pretrained(directory)'''"
      ],
      "metadata": {
        "id": "5_7ZFXpeKScr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df_train is a Series and tokenizer is already defined\n",
        "def split_text_into_chunks(text, max_length=500):\n",
        "    # Tokenize the full text first (without truncating)\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
        "\n",
        "    # Split into chunks of max_length\n",
        "    chunks = [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]\n",
        "    return chunks\n",
        "\n",
        "def get_token_length(text):\n",
        "    # Tokenize the text and return the number of tokens\n",
        "    return len(tokenizer.encode(text, add_special_tokens=True))\n",
        "\n",
        "# Create an empty list to collect new snippets\n",
        "new_snippets = []\n",
        "\n",
        "# Iterate over the Series (df_train)\n",
        "for text in df_train:\n",
        "    # Check the token length\n",
        "    token_length = get_token_length(text)\n",
        "\n",
        "    if token_length > 500:\n",
        "        # Split the text into chunks if it's too long\n",
        "        chunks = split_text_into_chunks(text)\n",
        "\n",
        "        # Add each chunk as a new entry in the new_snippets list\n",
        "        for chunk in chunks:\n",
        "            # Decode the chunk back into text\n",
        "            chunk_text = tokenizer.decode(chunk, skip_special_tokens=True)\n",
        "            new_snippets.append(chunk_text)\n",
        "    else:\n",
        "        # If the text is small enough, keep the original snippet\n",
        "        new_snippets.append(text)\n",
        "\n",
        "# Create a new Series with the updated snippets\n",
        "df_train_updated = pd.Series(new_snippets)\n",
        "\n",
        "# Show the updated Series\n",
        "print(df_train_updated)"
      ],
      "metadata": {
        "id": "AkfZnOqYorab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "def print_result(outputs):\n",
        "    table_data = []\n",
        "    for output in outputs:\n",
        "        token_str = f'\"{output[\"token_str\"]}\"'  # Preserve leading spaces by wrapping in quotes\n",
        "        table_data.append([output['sequence'], token_str, output['score']])\n",
        "\n",
        "    print(\"The suggested code completions are:\")\n",
        "    print(tabulate(table_data, headers=[\"Completion\", \"Predicted token\", \"Score\"], tablefmt=\"grid\", colalign=(\"left\", \"left\", \"center\")) )"
      ],
      "metadata": {
        "id": "8T-om7eYotuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "def code_completion(code_example, iterations: int = 1):\n",
        "    '''\n",
        "    - code_example: snipped of code that need to be code-completed. No token <mask> needed.\n",
        "    - iterations: number of subsequent code completions to be generated.\n",
        "                  Each sequence generated after the first one will be based only on the previous sequence with the highest score.\n",
        "    '''\n",
        "    code_example = code_example + \"<mask>\"\n",
        "    fill_mask = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n",
        "    current_example = code_example  # Start with the initial code\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        outputs = fill_mask(current_example)\n",
        "\n",
        "        # Take the first prediction and append <mask> to continue completion\n",
        "        best_prediction = outputs[0][\"sequence\"]\n",
        "        current_example = best_prediction + \" <mask>\"\n",
        "        print_result(outputs)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "OlO4tdwRo2eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = code_completion(\"def is_zero(x): return x==\")"
      ],
      "metadata": {
        "id": "gQYk4w7do35G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = code_completion(\"def add(a, b): return a\", 2)"
      ],
      "metadata": {
        "id": "NfiO6xN-pAV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = code_completion(\"for element \", 2)"
      ],
      "metadata": {
        "id": "mRibBlB5pBny"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}